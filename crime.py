import osimport cv2import torchimport torchvisionimport torch.nn as nnimport torch.optim as optimfrom torchvision import transforms, modelsfrom torch.utils.data import Dataset, DataLoaderfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_supportfrom tqdm import tqdmimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom flask import Flask, render_template_string, Response, jsonifyimport jsonimport smtplibfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartfrom datetime import datetimeimport threadingimport timeimport base64from io import BytesIOimport picklefrom collections import deque, Counter# ConfigurationVIDEO_DIR = r"C:\Users\manvi\Downloads\new_project"VIDEO_EXTENSIONS = ('.mp4', '.avi', '.mov', '.mkv', '.flv')MODEL_SAVE_PATH = "trained_model.pth"METRICS_SAVE_PATH = "training_metrics.json"EPOCHS = 3# Email ConfigurationEMAIL_SENDER = "manvifalwaria@gmail.com"EMAIL_RECEIVER = "famanvi0107@gmail.com"EMAIL_PASSWORD = "kzge quva qttz cjsa"  # App password# Flask Appapp = Flask(__name__)# HTML Template - Enhanced with better accuracy indicatorsHTML_TEMPLATE = """<!DOCTYPE html><html lang="en"><head>    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <title>Optimized Crime Detection System</title>    <style>        * {            margin: 0;            padding: 0;            box-sizing: border-box;        }        body {            font-family: 'Arial', sans-serif;            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);            min-height: 100vh;            color: #333;        }        .container {            max-width: 1200px;            margin: 0 auto;            padding: 20px;        }        .header {            text-align: center;            color: white;            margin-bottom: 30px;        }        .header h1 {            font-size: 3rem;            margin-bottom: 10px;            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);        }        .video-container {            display: flex;            justify-content: center;            margin-bottom: 30px;        }        .video-stream {            border: 5px solid white;            border-radius: 15px;            box-shadow: 0 10px 30px rgba(0,0,0,0.3);            max-width: 100%;            height: auto;        }        .controls {            display: flex;            justify-content: center;            gap: 20px;            margin-bottom: 30px;        }        .btn {            background: linear-gradient(45deg, #4CAF50, #45a049);            color: white;            border: none;            padding: 15px 30px;            border-radius: 25px;            cursor: pointer;            font-size: 18px;            font-weight: bold;            transition: all 0.3s ease;            box-shadow: 0 4px 15px rgba(0,0,0,0.2);        }        .btn:hover {            transform: translateY(-3px);            box-shadow: 0 6px 20px rgba(0,0,0,0.3);        }        .btn-danger {            background: linear-gradient(45deg, #f44336, #d32f2f);        }        .btn:disabled {            opacity: 0.6;            cursor: not-allowed;            transform: none;        }        .status-panel {            background: rgba(255, 255, 255, 0.9);            border-radius: 15px;            padding: 25px;            margin-bottom: 20px;            box-shadow: 0 10px 30px rgba(0,0,0,0.1);        }        .detection-log {            background: rgba(255, 255, 255, 0.9);            border-radius: 15px;            padding: 25px;            max-height: 300px;            overflow-y: auto;            box-shadow: 0 10px 30px rgba(0,0,0,0.1);        }        .log-entry {            padding: 10px;            border-bottom: 1px solid #eee;            font-family: monospace;        }        .log-entry.critical {            background: #ffebee;            border-left: 4px solid #f44336;            font-weight: bold;            color: #c62828;        }        .log-entry.high {            background: #fff3e0;            border-left: 4px solid #ff9800;            font-weight: bold;            color: #ef6c00;        }        .log-entry.medium {            background: #fffde7;            border-left: 4px solid #ffeb3b;            font-weight: bold;            color: #f57f17;        }        .log-entry.normal {            background: #e8f5e8;            border-left: 4px solid #4caf50;            color: #2e7d32;        }        .log-entry.suspicious {            background: #fff8e1;            border-left: 4px solid #ffc107;            color: #f57c00;        }        .confidence-bar {            width: 100%;            height: 20px;            background: #eee;            border-radius: 10px;            overflow: hidden;            margin: 10px 0;        }        .confidence-fill {            height: 100%;            transition: width 0.3s ease;        }        .confidence-high {            background: linear-gradient(90deg, #4caf50, #8bc34a);        }        .confidence-medium {            background: linear-gradient(90deg, #ff9800, #ffc107);        }        .confidence-low {            background: linear-gradient(90deg, #f44336, #ff5722);        }        .status-indicator {            display: inline-block;            width: 12px;            height: 12px;            border-radius: 50%;            margin-right: 8px;        }        .status-active {            background: #4CAF50;            animation: pulse 2s infinite;        }        .status-inactive {            background: #f44336;        }        @keyframes pulse {            0% { opacity: 1; }            50% { opacity: 0.5; }            100% { opacity: 1; }        }        .crime-alert {            background: linear-gradient(45deg, #ff4444, #cc0000);            color: white;            padding: 15px;            border-radius: 10px;            margin: 10px 0;            text-align: center;            font-weight: bold;            font-size: 18px;            animation: flashAlert 1s infinite;        }        @keyframes flashAlert {            0%, 100% { opacity: 1; }            50% { opacity: 0.7; }        }        .suspicious-alert {            background: linear-gradient(45deg, #ff8800, #ff6600);            color: white;            padding: 12px;            border-radius: 8px;            margin: 10px 0;            text-align: center;            font-weight: bold;        }    </style></head><body>    <div class="container">        <div class="header">            <h1>üéØ Optimized Crime Detection System</h1>            <p>AI-powered crime detection with smart context analysis</p>        </div>        <div class="video-container">            <img id="video-stream" class="video-stream" src="{{ url_for('video_feed') }}" alt="Video Stream">        </div>        <div class="controls">            <button class="btn" onclick="startDetection()" id="start-btn">üé• Start Detection</button>            <button class="btn btn-danger" onclick="stopDetection()" id="stop-btn" disabled>‚èπÔ∏è Stop Detection</button>        </div>        <div class="status-panel">            <h3>üìä System Status</h3>            <p><span id="status-indicator" class="status-indicator status-inactive"></span>               Detection Status: <span id="status-text">Inactive</span></p>            <p>Model: <span id="model-status">Loading...</span></p>            <p>Current Detection: <span id="last-detection">None</span></p>            <p>Confidence Score: <span id="confidence-score">N/A</span></p>            <p>Context Score: <span id="context-score">N/A</span></p>            <div class="confidence-bar">                <div id="confidence-fill" class="confidence-fill confidence-medium" style="width: 0%"></div>            </div>            <div id="crime-alert" style="display: none;"></div>            <div id="suspicious-alert" style="display: none;"></div>        </div>        <div class="detection-log">            <h3>üìù Detection Log (Context-Aware)</h3>            <div id="log-entries">                <div class="log-entry">Waiting for detection to start...</div>            </div>        </div>    </div>    <script>        let isDetecting = false;        let logUpdateInterval;        function startDetection() {            console.log('Starting optimized detection...');            fetch('/start_detection', { method: 'POST' })                .then(response => response.json())                .then(data => {                    console.log('Start detection response:', data);                    if (data.status === 'success') {                        isDetecting = true;                        document.getElementById('start-btn').disabled = true;                        document.getElementById('stop-btn').disabled = false;                        document.getElementById('status-indicator').className = 'status-indicator status-active';                        document.getElementById('status-text').textContent = 'Active - Optimized Mode';                        document.getElementById('model-status').textContent = 'Loaded & Running (Context-Aware)';                        // Start updating log                        logUpdateInterval = setInterval(updateLog, 1000);                    } else {                        alert('Failed to start detection: ' + data.message);                    }                })                .catch(error => {                    console.error('Error starting detection:', error);                    alert('Error starting detection. Check console for details.');                });        }        function stopDetection() {            console.log('Stopping detection...');            fetch('/stop_detection', { method: 'POST' })                .then(response => response.json())                .then(data => {                    console.log('Stop detection response:', data);                    isDetecting = false;                    document.getElementById('start-btn').disabled = false;                    document.getElementById('stop-btn').disabled = true;                    document.getElementById('status-indicator').className = 'status-indicator status-inactive';                    document.getElementById('status-text').textContent = 'Inactive';                    // Stop updating log                    clearInterval(logUpdateInterval);                    // Hide alerts                    document.getElementById('crime-alert').style.display = 'none';                    document.getElementById('suspicious-alert').style.display = 'none';                })                .catch(error => {                    console.error('Error stopping detection:', error);                });        }        function updateConfidenceBar(confidence) {            const fill = document.getElementById('confidence-fill');            const percentage = Math.round(confidence * 100);            fill.style.width = percentage + '%';            if (confidence >= 0.8) {                fill.className = 'confidence-fill confidence-high';            } else if (confidence >= 0.6) {                fill.className = 'confidence-fill confidence-medium';            } else {                fill.className = 'confidence-fill confidence-low';            }        }        function updateLog() {            fetch('/get_detections')                .then(response => response.json())                .then(data => {                    console.log('Detection data received:', data);                    const logContainer = document.getElementById('log-entries');                    const crimeAlert = document.getElementById('crime-alert');                    const suspiciousAlert = document.getElementById('suspicious-alert');                    if (data.detections && data.detections.length > 0) {                        const latest = data.detections[data.detections.length - 1];                        document.getElementById('last-detection').textContent =                             latest.display_class + ' (' + (latest.final_confidence * 100).toFixed(1) + '%)';                        document.getElementById('confidence-score').textContent =                             (latest.final_confidence * 100).toFixed(1) + '%';                        document.getElementById('context-score').textContent =                             (latest.context_score * 100).toFixed(1) + '%';                        updateConfidenceBar(latest.final_confidence);                        // Show appropriate alerts                        if (latest.is_verified_crime) {                            crimeAlert.innerHTML = 'üö® ' + latest.alert_level + ' ALERT: ' +                                 latest.display_class.toUpperCase() + ' - ' + (latest.final_confidence * 100).toFixed(1) + '% confidence';                            crimeAlert.style.display = 'block';                            suspiciousAlert.style.display = 'none';                        } else if (latest.is_suspicious) {                            suspiciousAlert.innerHTML = '‚ö†Ô∏è SUSPICIOUS ACTIVITY: ' +                                 latest.display_class + ' (' + (latest.final_confidence * 100).toFixed(1) + '%)';                            suspiciousAlert.style.display = 'block';                            crimeAlert.style.display = 'none';                        } else {                            crimeAlert.style.display = 'none';                            suspiciousAlert.style.display = 'none';                        }                        // Update log entries                        logContainer.innerHTML = data.detections.slice(-10).map(function(det) {                            let entryClass = 'log-entry';                            let alertIcon = '';                            if (det.is_verified_crime) {                                entryClass += ' ' + det.alert_level.toLowerCase();                                alertIcon = ' üî¥';                            } else if (det.is_suspicious) {                                entryClass += ' suspicious';                                alertIcon = ' ‚ö†Ô∏è';                            } else {                                entryClass += ' normal';                                alertIcon = ' ‚úÖ';                            }                            return '<div class="' + entryClass + '">' +                                det.timestamp + ' - ' + det.display_class + ' (' + (det.final_confidence * 100).toFixed(1) + '%) ' +                                '[Context: ' + (det.context_score * 100).toFixed(0) + '%]' + alertIcon +                            '</div>';                        }).join('');                        logContainer.scrollTop = logContainer.scrollHeight;                    }                })                .catch(error => {                    console.error('Error updating log:', error);                });        }        // Check model status on load        window.addEventListener('load', function() {            console.log('Page loaded, checking model status...');            fetch('/model_status')                .then(response => response.json())                .then(data => {                    console.log('Model status:', data);                    document.getElementById('model-status').textContent = data.status;                })                .catch(error => {                    console.error('Error getting model status:', error);                    document.getElementById('model-status').textContent = 'Error loading model';                });        });    </script></body></html>"""class OptimizedVideoClassificationSystem:    def __init__(self):        self.device = torch.device("cpu")        self.model = None        self.class_names = []        self.transform = None        self.training_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}        self.detection_log = []        self.is_detecting = False        self.camera = None        self.last_email_time = {}        # Optimized detection parameters        self.prediction_history = deque(maxlen=8)  # Moderate history        self.confidence_threshold = 0.8  # Reasonable threshold        self.temporal_window = 6  # Balanced window        self.consecutive_detections = deque(maxlen=12)        # Context-aware detection        self.motion_history = deque(maxlen=5)        self.previous_frame = None        # Refined crime detection with context awareness        self.explicit_violence_keywords = {            'fight', 'fighting', 'fist fight', 'physical fight', 'brawl',            'punch', 'punching', 'hit', 'hitting', 'kick', 'kicking',            'attack', 'attacking', 'assault', 'assaulting',            'violence', 'violent', 'aggressive', 'aggression'        }        self.weapon_keywords = {            'gun', 'shooting', 'gunshot', 'gunfire', 'pistol', 'rifle',            'knife', 'stabbing', 'blade', 'weapon', 'armed'        }        self.property_crime_keywords = {            'robbery', 'theft', 'stealing', 'burglary', 'break in',            'vandalism', 'destruction'        }        # Strong normal indicators that should override violence detection        self.strong_normal_indicators = {            'sitting', 'sitting down', 'seated', 'chair', 'desk',            'reading', 'book', 'study', 'studying', 'computer', 'laptop',            'phone', 'mobile', 'talking', 'conversation', 'indoor',            'bedroom', 'living room', 'office', 'home', 'peaceful',            'calm', 'relaxed', 'portrait', 'selfie', 'photo', 'pose'        }        self.setup_transform()    def setup_transform(self):        """Setup image preprocessing"""        self.transform = transforms.Compose([            transforms.ToPILImage(),            transforms.Resize((224, 224)),            transforms.ToTensor(),            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])        ])    def analyze_context(self, class_name, frame=None):        """Advanced context analysis to reduce false positives"""        if not class_name:            return 0.5, "No detection"        class_lower = class_name.lower().strip()        context_score = 0.5  # Neutral start        # Strong normal context indicators        normal_matches = sum(1 for indicator in self.strong_normal_indicators                             if indicator in class_lower)        if normal_matches > 0:            context_score = max(0.1, context_score - (normal_matches * 0.3))            return context_score, f"Strong normal context ({normal_matches} indicators)"        # Check for explicit violence indicators        violence_matches = sum(1 for keyword in self.explicit_violence_keywords                               if keyword in class_lower)        if violence_matches > 0:            context_score = min(0.9, context_score + (violence_matches * 0.2))        # Check for weapon indicators        weapon_matches = sum(1 for keyword in self.weapon_keywords                             if keyword in class_lower)        if weapon_matches > 0:            context_score = min(0.95, context_score + (weapon_matches * 0.3))        # Check for property crime indicators        property_matches = sum(1 for keyword in self.property_crime_keywords                               if keyword in class_lower)        if property_matches > 0:            context_score = min(0.8, context_score + (property_matches * 0.2))        # Analyze filename patterns for false positives        if any(ext in class_lower for ext in ['.mp4', '.avi', '.mov', '.mkv']):            # Common video naming patterns that suggest normal content            if any(pattern in class_lower for pattern in ['_normal', '_sitting', '_indoor', '_portrait']):                context_score = max(0.1, context_score - 0.4)                return context_score, "Filename suggests normal content"        # Motion analysis (if frame available)        if frame is not None:            motion_score = self.analyze_motion(frame)            if motion_score < 0.1:  # Very little motion                context_score = max(0.2, context_score - 0.3)                return context_score, "Low motion suggests static scene"        return context_score, "Context analysis complete"    def analyze_motion(self, current_frame):        """Analyze motion between frames to help with context"""        try:            gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)            if self.previous_frame is not None:                # Calculate frame difference                diff = cv2.absdiff(self.previous_frame, gray)                motion_pixels = np.sum(diff > 30)  # Threshold for significant change                total_pixels = diff.shape[0] * diff.shape[1]                motion_ratio = motion_pixels / total_pixels                self.motion_history.append(motion_ratio)                self.previous_frame = gray                # Return average motion over recent frames                return np.mean(list(self.motion_history))            else:                self.previous_frame = gray                return 0.5  # Neutral if no previous frame        except Exception as e:            print(f"Motion analysis error: {e}")            return 0.5    def smart_classification(self, class_name, confidence, context_score):        """Smart classification using confidence and context"""        if not class_name:            return "normal", "NORMAL", False, False, 0.5        class_lower = class_name.lower().strip()        # Combine confidence and context for final decision        adjusted_confidence = (confidence * 0.7) + (context_score * 0.3)        # If context strongly suggests normal activity, override        if context_score < 0.3:            return "normal", "NORMAL", False, False, adjusted_confidence        # Check for explicit violence with high requirements        explicit_violence = any(keyword in class_lower for keyword in self.explicit_violence_keywords)        weapon_detected = any(keyword in class_lower for keyword in self.weapon_keywords)        if weapon_detected and adjusted_confidence >= 0.85:            return "crime", "CRITICAL", True, False, adjusted_confidence        elif explicit_violence and adjusted_confidence >= 0.82:            return "crime", "HIGH", True, False, adjusted_confidence        elif any(keyword in class_lower for keyword in self.property_crime_keywords) and adjusted_confidence >= 0.78:            return "crime", "MEDIUM", True, False, adjusted_confidence        elif (explicit_violence or weapon_detected) and adjusted_confidence >= 0.65:            return "suspicious", "SUSPICIOUS", False, True, adjusted_confidence        elif adjusted_confidence >= 0.6 and context_score > 0.6:            return "suspicious", "SUSPICIOUS", False, True, adjusted_confidence        else:            return "normal", "NORMAL", False, False, adjusted_confidence    def enhanced_temporal_smoothing(self, current_prediction, current_confidence, context_score):        """Enhanced temporal smoothing with context awareness"""        # Add current prediction        self.prediction_history.append({            'class': current_prediction,            'confidence': current_confidence,            'context_score': context_score,            'timestamp': time.time()        })        if len(self.prediction_history) < 4:            return current_prediction, current_confidence, context_score, 0.0        # Get recent predictions        recent_predictions = list(self.prediction_history)[-self.temporal_window:]        # Count class occurrences        class_counts = Counter([pred['class'] for pred in recent_predictions])        most_common_class, occurrence_count = class_counts.most_common(1)[0]        # Calculate stability        stability_score = occurrence_count / len(recent_predictions)        # Get metrics for the most common class        relevant_predictions = [pred for pred in recent_predictions if pred['class'] == most_common_class]        if relevant_predictions:            # Weighted averages (recent predictions have more weight)            weights = np.linspace(0.6, 1.0, len(relevant_predictions))            smoothed_confidence = np.average([p['confidence'] for p in relevant_predictions], weights=weights)            smoothed_context = np.average([p['context_score'] for p in relevant_predictions], weights=weights)        else:            smoothed_confidence = current_confidence            smoothed_context = context_score        return most_common_class, smoothed_confidence, smoothed_context, stability_score    def get_display_name(self, class_name, activity_type, confidence):        """Get user-friendly display name"""        if not class_name:            return "No Detection"        if activity_type == "normal":            return "Safe Environment"        elif activity_type == "suspicious":            # Clean up class name for suspicious activities            display_name = class_name.replace('.mp4', '').replace('.avi', '').replace('.mov', '').replace('.mkv', '')            display_name = display_name.replace('_x264', '').replace('_', ' ')            return f"Suspicious: {' '.join(word.capitalize() for word in display_name.split())}"        else:  # crime            # Clean up class name for crimes            display_name = class_name.replace('.mp4', '').replace('.avi', '').replace('.mov', '').replace('.mkv', '')            display_name = display_name.replace('_x264', '').replace('_', ' ')            return f"CRIME: {' '.join(word.capitalize() for word in display_name.split())}"    def load_model(self, model_path=MODEL_SAVE_PATH):        """Load trained model"""        try:            if not os.path.exists(model_path):                print(f"‚ùå Model file not found: {model_path}")                return False            print(f"üì• Loading optimized model from {model_path}...")            checkpoint = torch.load(model_path, map_location=self.device)            if 'class_names' in checkpoint:                self.class_names = checkpoint['class_names']                print(f"‚úÖ Found {len(self.class_names)} classes in model")            else:                print("‚ùå No class names found in checkpoint")                return False            # Create model architecture            model = models.mobilenet_v2(weights=None)            model.classifier[1] = nn.Linear(model.last_channel, len(self.class_names))            if 'model_state_dict' in checkpoint:                model.load_state_dict(checkpoint['model_state_dict'])                print("‚úÖ Model weights loaded successfully")            else:                print("‚ùå No model state dict found in checkpoint")                return False            model = model.to(self.device)            model.eval()            self.model = model            self.training_history = checkpoint.get('training_history', {})            print("‚úÖ Optimized model loaded and ready!")            return True        except Exception as e:            print(f"‚ùå Error loading model: {str(e)}")            return False    def send_email_alert(self, subject, message):        """Send email alert with rate limiting"""        try:            current_time = time.time()            alert_type = subject.split(':')[1].split('-')[0].strip() if ':' in subject else 'unknown'            # Rate limiting - 45 seconds between similar alerts            if alert_type in self.last_email_time:                if current_time - self.last_email_time[alert_type] < 45:                    print(f"Email rate limited for {alert_type}")                    return False            self.last_email_time[alert_type] = current_time            msg = MIMEMultipart()            msg['From'] = EMAIL_SENDER            msg['To'] = EMAIL_RECEIVER            msg['Subject'] = subject            msg.attach(MIMEText(message, 'plain'))            server = smtplib.SMTP('smtp.gmail.com', 587)            server.starttls()            server.login(EMAIL_SENDER, EMAIL_PASSWORD)            text = msg.as_string()            server.sendmail(EMAIL_SENDER, EMAIL_RECEIVER, text)            server.quit()            print("‚úÖ Optimized crime alert sent!")            return True        except Exception as e:            print(f"‚ùå Failed to send email: {str(e)}")            return False    def process_frame_for_detection(self, frame):        """Process frame with optimized detection"""        try:            if self.model is None:                cv2.putText(frame, "Model not loaded", (10, 30),                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)                return frame            # Preprocess frame            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)            img = cv2.resize(img, (224, 224))            if self.transform is None:                self.setup_transform()            tensor = self.transform(img).unsqueeze(0).to(self.device)            with torch.no_grad():                output = self.model(tensor)                probabilities = torch.nn.functional.softmax(output[0], dim=0)                confidence, pred = torch.max(probabilities, 0)                if pred.item() >= len(self.class_names):                    print(f"Warning: Invalid prediction index {pred.item()}")                    cv2.putText(frame, "Invalid prediction", (10, 30),                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)                    return frame                raw_label = self.class_names[pred.item()]                raw_confidence = confidence.item()            # Analyze context            context_score, context_reason = self.analyze_context(raw_label, frame)            # Apply temporal smoothing            smoothed_label, smoothed_confidence, smoothed_context, stability_score = self.enhanced_temporal_smoothing(                raw_label, raw_confidence, context_score)            # Smart classification            activity_type, alert_level, is_verified_crime, is_suspicious, final_confidence = self.smart_classification(                smoothed_label, smoothed_confidence, smoothed_context)            # Get display name            display_class = self.get_display_name(smoothed_label, activity_type, final_confidence)            # Detection info            detection_info = {                'timestamp': datetime.now().strftime('%H:%M:%S'),                'class': raw_label,                'display_class': display_class,                'confidence': raw_confidence,                'final_confidence': final_confidence,                'context_score': smoothed_context,                'stability_score': stability_score,                'is_verified_crime': is_verified_crime,                'is_suspicious': is_suspicious,                'alert_level': alert_level,                'activity_type': activity_type,                'context_reason': context_reason            }            # Add to logs            self.detection_log.append(detection_info)            self.consecutive_detections.append(detection_info)            # Trim logs            if len(self.detection_log) > 50:                self.detection_log.pop(0)            # Send email for verified crimes with strict requirements            if (is_verified_crime and                    final_confidence >= 0.85 and                    stability_score >= 0.7 and                    smoothed_context >= 0.6):                self.send_email_alert(                    f"üö® {alert_level} CRIME ALERT: {display_class.upper()}",                    f"OPTIMIZED CRIME DETECTION ALERT!\n\n"                    f"üïí Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"                    f"üéØ Activity: {display_class}\n"                    f"üìä Final Confidence: {final_confidence:.1%}\n"                    f"üîç Context Score: {smoothed_context:.1%}\n"                    f"üéØ Stability: {stability_score:.1%}\n"                    f"‚ö†Ô∏è Alert Level: {alert_level}\n"                    f"üìù Context Analysis: {context_reason}\n"                )            # Frame annotation with context awareness            if is_verified_crime:                if alert_level == "CRITICAL":                    color = (0, 0, 255)  # Red                    bg_color = (0, 0, 139)                elif alert_level == "HIGH":                    color = (0, 69, 255)  # Orange-red                    bg_color = (0, 100, 139)                else:                    color = (0, 165, 255)  # Orange                    bg_color = (0, 100, 100)                thickness = 3            elif is_suspicious:                color = (0, 255, 255)  # Yellow                bg_color = (0, 139, 139)                thickness = 2            else:                color = (0, 255, 0)  # Green                bg_color = (0, 100, 0)                thickness = 2            # Main detection text            main_text = f"{display_class}: {final_confidence:.1%}"            text_size = cv2.getTextSize(main_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, thickness)[0]            cv2.rectangle(frame, (5, 5), (text_size[0] + 15, 32), bg_color, -1)            cv2.putText(frame, main_text, (10, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness)            # Context and stability scores            metrics_text = f"Context: {smoothed_context:.1%} | Stability: {stability_score:.1%}"            cv2.rectangle(frame, (5, 35), (350, 55), (50, 50, 50), -1)            cv2.putText(frame, metrics_text, (10, 47), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)            # Status indicator            if is_verified_crime:                status_text = f"VERIFIED {alert_level} CRIME"                cv2.rectangle(frame, (5, 60), (300, 85), (0, 0, 255), -1)                cv2.putText(frame, status_text, (10, 77), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)                # Add flashing border for critical crimes                if alert_level == "CRITICAL":                    border_color = (0, 0, 255) if int(time.time() * 3) % 2 else (255, 255, 255)                    cv2.rectangle(frame, (3, 3), (frame.shape[1] - 3, frame.shape[0] - 3), border_color, 6)            elif is_suspicious:                sus_text = "SUSPICIOUS - MONITORING"                cv2.rectangle(frame, (5, 60), (260, 85), (0, 255, 255), -1)                cv2.putText(frame, sus_text, (10, 77), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)            else:                safe_text = "ENVIRONMENT SAFE"                cv2.rectangle(frame, (5, 60), (200, 85), (0, 150, 0), -1)                cv2.putText(frame, safe_text, (10, 77), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)            # Context reason (small text)            if len(context_reason) < 40:                cv2.rectangle(frame, (5, 90), (min(400, len(context_reason) * 8), 110), (100, 100, 100), -1)                cv2.putText(frame, context_reason[:50], (10, 102), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)        except Exception as e:            print(f"Detection error: {e}")            cv2.putText(frame, f"Detection Error: {str(e)[:30]}", (10, 30),                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)        return frame    def generate_frames(self):        """Generate video frames for streaming"""        if self.camera is None:            self.camera = cv2.VideoCapture(0)            if not self.camera.isOpened():                for i in range(1, 4):                    self.camera = cv2.VideoCapture(i)                    if self.camera.isOpened():                        break            if self.camera.isOpened():                self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)                self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)                self.camera.set(cv2.CAP_PROP_FPS, 20)        while True:            if self.camera is None or not self.camera.isOpened():                frame = np.zeros((480, 640, 3), dtype=np.uint8)                cv2.putText(frame, "Camera not available", (50, 240),                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)            else:                success, frame = self.camera.read()                if not success:                    frame = np.zeros((480, 640, 3), dtype=np.uint8)                    cv2.putText(frame, "Failed to read camera", (50, 240),                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)                else:                    if self.model is not None and self.is_detecting:                        frame = self.process_frame_for_detection(frame)            ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])            if ret:                frame_bytes = buffer.tobytes()                yield (b'--frame\r\n'                       b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')# Global system instanceclassification_system = OptimizedVideoClassificationSystem()# Flask Routes@app.route('/')def index():    return render_template_string(HTML_TEMPLATE)@app.route('/video_feed')def video_feed():    return Response(classification_system.generate_frames(),                    mimetype='multipart/x-mixed-replace; boundary=frame')@app.route('/start_detection', methods=['POST'])def start_detection():    try:        if classification_system.model is None:            if not classification_system.load_model():                return jsonify({                    'status': 'error',                    'message': 'No trained model available. Please ensure trained_model.pth exists.'                })        classification_system.is_detecting = True        print("‚úÖ Optimized detection started!")        return jsonify({'status': 'success', 'message': 'Optimized detection started'})    except Exception as e:        print(f"‚ùå Error starting detection: {str(e)}")        return jsonify({'status': 'error', 'message': f'Error: {str(e)}'})@app.route('/stop_detection', methods=['POST'])def stop_detection():    try:        classification_system.is_detecting = False        classification_system.prediction_history.clear()        classification_system.consecutive_detections.clear()        classification_system.motion_history.clear()        classification_system.previous_frame = None        print("üõë Optimized detection stopped")        return jsonify({'status': 'success', 'message': 'Detection stopped'})    except Exception as e:        print(f"‚ùå Error stopping detection: {str(e)}")        return jsonify({'status': 'error', 'message': f'Error: {str(e)}'})@app.route('/model_status')def model_status():    try:        if classification_system.model is None:            success = classification_system.load_model()            if not success:                return jsonify({                    'status': 'Model not found - Please ensure trained_model.pth exists',                    'loaded': False,                    'class_count': 0                })        if hasattr(classification_system, 'class_names') and len(classification_system.class_names) > 0:            status = f'Optimized Model Loaded ({len(classification_system.class_names)} classes)'            return jsonify({                'status': status,                'loaded': True,                'class_count': len(classification_system.class_names)            })        else:            return jsonify({                'status': 'Model loaded but no classes found',                'loaded': False,                'class_count': 0            })    except Exception as e:        return jsonify({            'status': f'Error: {str(e)}',            'loaded': False,            'class_count': 0        })@app.route('/get_detections')def get_detections():    try:        recent_detections = classification_system.detection_log[-20:] if classification_system.detection_log else []        return jsonify({'detections': recent_detections})    except Exception as e:        print(f"‚ùå Error getting detections: {str(e)}")        return jsonify({'detections': []})@app.route('/debug_info')def debug_info():    """Enhanced debug endpoint with context analysis"""    try:        info = {            'model_loaded': classification_system.model is not None,            'class_count': len(classification_system.class_names) if classification_system.class_names else 0,            'is_detecting': classification_system.is_detecting,            'detection_log_count': len(classification_system.detection_log),            'prediction_history_count': len(classification_system.prediction_history),            'motion_history_count': len(classification_system.motion_history),            'temporal_window': classification_system.temporal_window,            'confidence_threshold': classification_system.confidence_threshold,            'camera_active': classification_system.camera is not None and classification_system.camera.isOpened() if classification_system.camera else False,            'model_file_exists': os.path.exists(MODEL_SAVE_PATH),            'recent_detections': classification_system.detection_log[                                 -3:] if classification_system.detection_log else [],            'explicit_violence_keywords': list(classification_system.explicit_violence_keywords),            'weapon_keywords': list(classification_system.weapon_keywords),            'strong_normal_indicators': list(classification_system.strong_normal_indicators)        }        return jsonify(info)    except Exception as e:        return jsonify({'error': str(e)})def main():    print("üéØ Optimized Crime Detection System Starting...")    print(f"üì± Device: {classification_system.device}")    print(f"üìÅ Model path: {MODEL_SAVE_PATH}")    print("üß† Optimized Features:")    print("   - Context-aware analysis")    print("   - Motion detection integration")    print("   - Smart false positive reduction")    print("   - Multi-factor crime verification")    print("   - 80% confidence threshold with context weighting")    if os.path.exists(MODEL_SAVE_PATH):        print("üì• Model file found. Loading...")        if classification_system.load_model():            print("‚úÖ Optimized model loaded successfully!")            print(f"üìä Model has {len(classification_system.class_names)} classes")            if len(classification_system.class_names) > 0:                print("üìã Class analysis with context awareness:")                for i, class_name in enumerate(classification_system.class_names[:8]):                    context_score, context_reason = classification_system.analyze_context(class_name)                    activity_type, alert_level, is_crime, is_suspicious, final_conf = classification_system.smart_classification(                        class_name, 0.85, context_score)                    display_name = classification_system.get_display_name(class_name, activity_type, final_conf)                    if is_crime:                        status = f"[{alert_level} CRIME]"                    elif is_suspicious:                        status = "[SUSPICIOUS]"                    else:                        status = "[SAFE]"                    print(f"   {i}: {class_name}")                    print(f"      -> {display_name} {status}")                    print(f"      -> Context: {context_score:.2f} ({context_reason})")                if len(classification_system.class_names) > 8:                    print(f"   ... and {len(classification_system.class_names) - 8} more classes")        else:            print("‚ùå Failed to load model")    else:        print(f"‚ùå Model file not found: {MODEL_SAVE_PATH}")    print("\nüåê Starting optimized web server...")    print("üé• Open your browser and go to:")    print("   - http://localhost:5000 (local access)")    print("   - http://192.168.1.39:5000 (network access)")    print("üìß Email alerts for context-verified crimes")    print("üîç Debug info: http://localhost:5000/debug_info")    print("\nüß† Key Optimizations:")    print("   - Strong normal indicators override violence detection")    print("   - Motion analysis for static scene detection")    print("   - Context-weighted confidence scoring")    print("   - Multi-factor verification for crime alerts")    print("   - Filename pattern analysis")    try:        app.run(debug=False, host='0.0.0.0', port=5000, threaded=True)    except KeyboardInterrupt:        print("\nüõë Shutting down optimized system...")        if classification_system.camera is not None:            classification_system.camera.release()        cv2.destroyAllWindows()    except Exception as e:        print(f"‚ùå Error starting server: {str(e)}")if __name__ == "__main__":    main()